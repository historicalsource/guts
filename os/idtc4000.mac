; $Id: idtc4000.mac,v 1.20 1997/07/01 20:54:19 shepperd Exp $
;
/*
 * R4000 cache operations
 */

	.include	config.mac

.if not_defined, USE_MIPS4
USE_MIPS4 == 0
.endc
.if not_defined, USE_MIPS3
USE_MIPS3 == 1
.endc
.if true, USE_MIPS4
	.set mips4
.iff
 .if true, USE_MIPS3
	.set mips3
 .endc
.endc

HAS_SCACHE == 0

.if not_defined, BOOT_ROM_CODE
BOOT_ROM_CODE == 0
.endc

	.macro ALIGN3
	.align 3
	.endm

.if defined,TINY_MODE
 .if true, TINY_MODE > 0
MIN_IDTC4000 == 1
 .endc
.endc

.if not_defined, MIN_IDTC4000
MIN_IDTC4000 == 0
.endc

.if not_defined, CACHE_MEM_BASE
CACHE_MEM_BASE == 0
CACHE_MEM_BASE = PROM_BASE
.endc

.macro	LEAF label args
	FRAME label args
.endm

.macro	END label
	ENDFRAME label
.endm

/*
 * cacheop macro to automate cache operations
 * first some helpers...
 */
.macro	_mincache size, maxsize
	bltu	size,maxsize,8f
	move	size,maxsize
	ALIGN3
8:
.endm

.macro	_align tmp, minaddr, maxaddr, linesize
	subu	tmp,linesize,1
	not	tmp 
	and	minaddr,tmp
	addu	maxaddr,-1
	and	maxaddr,tmp
.endm

/* This is a bit of a hack really because it relies on minaddr=a0 */
.macro	_doop1 op1
	cache	op1,0(a0)
	nop
.endm

.macro	_doop2 op1, op2
	cache	op1,0(a0)
	nop
	cache	op2,0(a0)
	nop
.endm

/* specials for cache initialisation */
.macro	_doop1lw1 op1
	cache	op1,0(a0)
	nop
	lw	zero,0(a0)
	nop
	cache	op1,0(a0)
	nop
.endm

.macro	_doop121 op1, op2
	cache	op1,0(a0)
	nop
	cache	op2,0(a0) 
	nop
	cache	op1,0(a0)
.endm

.macro	_oploopn minaddr, maxaddr, linesize, tag, ops
	.set	noreorder
	ALIGN3
7:
	_doop'tag	ops
	bne     minaddr,maxaddr,7b
	addu   	minaddr,linesize
	.set	reorder
.endm

/* finally the cache operation macros */
.macro	icacheopn kva, n, cache_size, cache_linesize, tag, ops
	_mincache n, cache_size
 	blez	n, 9f
	addu	n, kva
	_align	t1, kva, n, cache_linesize
	_oploopn kva, n, cache_linesize, tag, <ops>
	ALIGN3
9:
.endm

.macro	vcacheopn kva, n, cache_size, cache_linesize, tag, ops
 	blez	n,9f
	addu	n,kva
	_align	t1, kva, n, cache_linesize
	_oploopn kva, n, cache_linesize, tag, <ops>
	ALIGN3
9:
.endm

.macro	icacheop kva, n, cache_size, cache_linesize, op
	icacheopn kva, n, cache_size, cache_linesize, 1, <op>
.endm

.macro	vcacheop kva, n, cache_size, cache_linesize, op
	vcacheopn kva, n, cache_size, cache_linesize, 1, <op>
.endm

.if true, BOOT_ROM_CODE == 0
	.bss
	.globl	icache_size; .globl icache_linesize
icache_size:
	.space	4
icache_linesize:
	.space	4
	.globl	dcache_size; .globl dcache_linesize
dcache_size:
	.space	4
dcache_linesize:
	.space	4
.if true, HAS_SCACHE
	.globl	scache_size; .globl scache_linesize
scache_size:
	.space	4
scache_linesize:
	.space	4
.endc
.endc
	.text
/*
 * static void size_cache()
 * 
 * Internal routine to determine cache sizes by looking at R4000 config
 * register.  Sizes are returned in registers, as follows:
 *	t2	icache size
 *	t3	dcache size
 *	t6	scache size
 *	t4	icache line size
 *	t5	dcache line size
 *	t7	scache line size
 */
	ALIGN3
LEAF size_cache 
	mfc0	t0,C0_CONFIG

	and	t1,t0,CFG_ICMASK
	srl	t1,CFG_ICSHIFT
	li	t2,0x1000
	sll	t2,t1

	and	t1,t0,CFG_DCMASK
	srl	t1,CFG_DCSHIFT
	li	t3,0x1000
	sll	t3,t1

	li	t4,32
	and	t1,t0,CFG_IB
	bnez	t1,1f
	li	t4,16
	ALIGN3
1:	

	li	t5,32
	and	t1,t0,CFG_DB
	bnez	t1,1f
	li	t5,16
	ALIGN3
1:	

	move	t6,zero			# default to no scache
	move	t7,zero			#

	and	t1,t0,CFG_C_UNCACHED	# test config register
	bnez	t1,1f			# no scache if uncached/non-coherent
	
	li	t6,0x100000		# assume 1Mb scache <<-NOTE
	and	t1,t0,CFG_SBMASK
	srl	t1,CFG_SBSHIFT
	li	t7,16
	sll	t7,t1
	ALIGN3
1:	j	ra
END size_cache

.if true, BOOT_ROM_CODE == 0
/*
 * void config_cache()
 * 
 * Work out size of I, D & S caches, assuming they are already initialised.
 */
	ALIGN3
LEAF config_cache
	lw	t0,icache_size
	bgtz	t0,8f			# already known?
	move	v0,ra
	ALIGN3
	bal	size_cache
	move	ra,v0

	sw	t2,icache_size
	sw	t3,dcache_size
	and	t4, 0xFF
	sw	t4,icache_linesize
	and	t5, 0xFF
	sw	t5,dcache_linesize
.if true, HAS_SCACHE
	sw	t6,scache_size
	and	t7, 0xFF
	sw	t7,scache_linesize
.endc
	ALIGN3
8:	j	ra
END config_cache
.endc

/*
 * void init_cache()
 */
	ALIGN3
LEAF init_cache
	/*
 	 * First work out the sizes
	 */
	move	v0,ra
	ALIGN3
	bal	size_cache
	move	ra,v0
	
	/*
	 * The caches may be in an indeterminate state,
	 * so we force good parity into them by doing an
	 * invalidate, load/fill, invalidate for each line.
	 */

	/* disable all i/u and cache exceptions */
	mfc0	v0,C0_SR
	and	v1,v0,~SR_IE
	or	v1,SR_DE
	mtc0	v1,C0_SR

	mtc0	zero,C0_TAGLO
	mtc0	zero,C0_TAGHI

	/* assume bottom of RAM will generate good parity for the cache */
	li	a0, CACHE_MEM_BASE
	move	a2,t2		# icache_size
	move	a3,t4		# icache_linesize
	move	a1,a2
	icacheopn a0, a1, a2, a3, 121, <Index_Store_Tag_I, Fill_I>

	li	a0, CACHE_MEM_BASE
	move	a2,t3		# dcache_size
	move	a3,t5		# dcache_linesize
	move	a1,a2
	icacheopn a0, a1, a2, a3, 1lw1, <Index_Store_Tag_D>

.if true, HAS_SCACHE		; only if there is an scache
	/* assume unified I & D in scache <<-NOTE */
	blez	t6,1f
	li	a0, CACHE_MEM_BASE
	move	a2,t6
	move	a3,t7
	move	a1,a2
	icacheopn a0, a1, a2, a3, 1lw1, <Index_Store_Tag_SD>

	ALIGN3
1:
.endc
	mtc0	v0,C0_SR
	j	ra
END init_cache
	
.if true, BOOT_ROM_CODE == 0
/*
 * void flush_cache (void)
 *
 * Flush and invalidate all caches
 */
DIFF = DRAM_BASEnc - DRAM_BASE
	ALIGN3
LEAF flush_cache
.if true, (HOST_BOARD == HCR4K) || (HOST_BOARD == MB4600)
	la	v0, 30f			;# get ptr to entry point
	srl	a1, v0, 31		;# get the upper bit of address
	bne	a1, r0, 10f		;# if set, we're running in 9fx (EPROM)
	addu	a0, v0, DIFF		;# else we're running out of DRAM, so make address non-cached
	b	20f

	ALIGN3
10:	or	a0, v0, 0xBF000000	;# else switch to non-cached EPROM
	ALIGN3
20:	j	a0
.endc
.if true, (HOST_BOARD == PHOENIX) || (HOST_BOARD == PHOENIX_AD) || (HOST_BOARD == FLAGSTAFF) || (HOST_BOARD == SEATTLE) || (HOST_BOARD == VEGAS)
	la	v0, 30f
	or	v0, 0x20000000		;# make sure we are running non-cached
	j	v0
	nop
.endc
	ALIGN3
30:
.if true, HAS_SCACHE
	/* secondary cacheops do all the work if present */
	lw	a2,scache_size
	blez	a2, 40f
	lw	a3,scache_linesize
	li	a0, CACHE_MEM_BASE
	move	a1,a2
	icacheop a0, a1, a2, a3, Index_Writeback_Inv_SD
	b	50f

	ALIGN3
40:
.endc
	lw	a2,icache_size
	blez	a2, 50f
	lw	a3,icache_linesize
	li	a0, CACHE_MEM_BASE
	move	a1,a2
	icacheop a0, a1, a2, a3, Index_Invalidate_I>

	lw	a2,dcache_size
	lw	a3,dcache_linesize
	li	a0, CACHE_MEM_BASE
	move	a1,a2
	icacheop a0, a1, a2, a3, Index_Writeback_Inv_D

	ALIGN3
50:	j	ra
END flush_cache
	
/*
 * void flush_dcache (void)
 *
 * Flush and invalidate data caches
 */
	.align 3
LEAF flush_dcache
.if true, HAS_SCACHE
	/* secondary cacheops do all the work if present */
	lw	a2,scache_size
	blez	a2, 40f
	lw	a3,scache_linesize
	li	a0, CACHE_MEM_BASE
	move	a1,a2
	icacheop a0, a1, a2, a3, Index_Writeback_Inv_SD
	b	50f

	ALIGN3
40:
.endc
	lw	a2,dcache_size
	lw	a3,dcache_linesize
	li	a0, CACHE_MEM_BASE
	move	a1,a2
	icacheop a0, a1, a2, a3, Index_Writeback_Inv_D

	ALIGN3
50:	j	ra
END flush_dcache
	
.if true, MIN_IDTC4000 == 0
/*
 * void flush_cache_nowrite (void)
 *
 * Invalidate all caches
 */
	ALIGN3
LEAF flush_cache_nowrite
	mfc0	v0,C0_SR
	and	v1,v0,~SR_IE
	mtc0	v1,C0_SR

	mtc0	zero,C0_TAGLO
	mtc0	zero,C0_TAGHI

	lw	a2,icache_size
	blez	a2,2f
	lw	a3,icache_linesize
	li	a0, CACHE_MEM_BASE
	move	a1,a2
	icacheop a0, a1, a2, a3, Index_Invalidate_I

	lw	a2,dcache_size
	lw	a3,dcache_linesize
	li	a0, CACHE_MEM_BASE
	move	a1,a2
	icacheop a0, a1, a2, a3, Index_Store_Tag_D

.if true, HAS_SCACHE
	lw	a2,scache_size
	blez	a2,2f
	lw	a3,scache_linesize
	li	a0, CACHE_MEM_BASE
	move	a1,a2
	icacheop a0, a1, a2, a3, Index_Store_Tag_SD

.endc
	ALIGN3
2:
	mtc0	v0,C0_SR
	j	ra
END flush_cache_nowrite
	
/*
 * void clear_cache (unsigned kva, size_t n)
 *
 * Writeback and invalidate address range in all caches
 */
	ALIGN3
LEAF clear_cache 
.if true, HAS_SCACHE
	/* secondary cacheops do all the work (if fitted) */
	lw	a2,scache_size
	blez	a2,1f
	lw	a3,scache_linesize
	vcacheop a0, a1, a2, a3, Hit_Writeback_Inv_SD
	b	2f

	ALIGN3
1:
.endc
	lw	a2,icache_size
	blez	a2,2f
	lw	a3,icache_linesize
	/* save kva & n for subsequent loop */
	move	t8,a0
	move	t9,a1
	vcacheop a0, a1, a2, a3, Hit_Invalidate_I

	lw	a2,dcache_size
	lw	a3,dcache_linesize
	/* restore kva & n */
	move	a0,t8
	move	a1,t9
	vcacheop a0, a1, a2, a3, Hit_Writeback_Inv_D

	ALIGN3
2:	j	ra
END clear_cache
	
/*
 * void clean_dcache (unsigned kva, size_t n)
 *
 * Writeback and invalidate address range in primary data cache
 */
	ALIGN3
LEAF clean_dcache
	lw	a2,dcache_size
	blez	a2,2f
	lw	a3,dcache_linesize

	vcacheop a0, a1, a2, a3, Hit_Writeback_Inv_D

	ALIGN3
2:	j	ra
END clean_dcache
	
/*
 * void clean_dcache_indexed (unsigned kva, size_t n)
 *
 * Writeback and invalidate indexed range in primary data cache
 */
	ALIGN3
LEAF clean_dcache_indexed
	lw	a2,dcache_size
	blez	a2,2f
	lw	a3,dcache_linesize

	srl	a2,1			# do one set (half cache) at a time
	move	t8,a0			# save kva & n
	move	t9,a1
	icacheop a0, a1, a2, a3, Index_Writeback_Inv_D

	addu	a0,t8,a2		# do next set
	move	a1,t9			# restore n
	icacheop a0, a1, a2, a3, Index_Writeback_Inv_D

	ALIGN3
2:	j	ra
END clean_dcache_indexed
	
/*
 * void clean_dcache_nowrite (unsigned kva, size_t n)
 *
 * Invalidate an address range in primary data cache
 */
	ALIGN3
LEAF clean_dcache_nowrite
	lw	a2,dcache_size
	blez	a2,2f
	lw	a3,dcache_linesize

	vcacheop a0, a1, a2, a3, Hit_Invalidate_D

	ALIGN3
2:	j	ra
END clean_dcache_nowrite
	
/*
 * void clean_dcache_nowrite_indexed (unsigned kva, size_t n)
 *
 * Invalidate indexed range in primary data cache
 */
	ALIGN3
LEAF clean_dcache_nowrite_indexed
	mfc0	v0,C0_SR
	and	v1,v0,~SR_IE
	mtc0	v1,C0_SR

	mtc0	zero,C0_TAGLO
	mtc0	zero,C0_TAGHI

	lw	a2,dcache_size
	blez	a2,2f
	lw	a3,dcache_linesize

	srl	a2,1			# do one set (half cache) at a time
	move	t8,a0			# save kva & n
	move	t9,a1
	icacheop a0, a1, a2, a3, Index_Store_Tag_D

	addu	a0,t8,a2		# do next set
	move	a1,t9			# restore n
	icacheop a0, a1, a2, a3, Index_Store_Tag_D

	ALIGN3
2:	mtc0	v0,C0_SR
	j	ra
END clean_dcache_nowrite_indexed
	
/*
 * void clean_icache (unsigned kva, size_t n)
 *
 * Invalidate address range in primary instruction cache
 */
	ALIGN3
LEAF clean_icache
	lw	a2,icache_size
	blez	a2,2f
	lw	a3,icache_linesize

	vcacheop a0, a1, a2, a3, Hit_Invalidate_I

	ALIGN3
2:	j	ra
END clean_icache
	
/*
 * void clean_icache_indexed (unsigned kva, size_t n)
 *
 * Invalidate indexed range in primary instruction cache
 */
	ALIGN3
LEAF clean_icache_indexed
	lw	a2,icache_size
	blez	a2,2f
	lw	a3,icache_linesize

	srl	a2,1			# do one set (half cache) at a time
	move	t8,a0			# save kva & n
	move	t9,a1
	icacheop a0, a1, a2, a3, Index_Invalidate_I

	addu	a0,t8,a2		# do next set
	move	a1,t9			# restore n
	icacheop a0, a1, a2, a3, Index_Invalidate_I

	ALIGN3
2:	j	ra
END clean_icache_indexed
	
.if true, HAS_SCACHE			; No scaches in our hardware
/*
 * void clean_scache (unsigned kva, size_t n)
 *
 * Writeback and invalidate address range in secondary cache
 */
	ALIGN3
LEAF clean_scache
	lw	a2,scache_size
	blez	a2,2f
	lw	a3,scache_linesize
	vcacheop a0, a1, a2, a3, Hit_Writeback_Inv_SD

	ALIGN3
2:	j	ra
END clean_scache
	
/*
 * void clean_scache_indexed (unsigned kva, size_t n)
 *
 * Writeback and invalidate indexed range in secondary cache
 */
	ALIGN3
LEAF clean_scache_indexed
	lw	a2,scache_size
	blez	a2,2f
	lw	a3,scache_linesize

	icacheop a0, a1, a2, a3, Index_Writeback_Inv_SD

	ALIGN3
2:	j	ra
END clean_scache_indexed
	
/*
 * void clean_scache_nowrite (unsigned kva, size_t n)
 *
 * Invalidate an address range in secondary cache
 */
	ALIGN3
LEAF clean_scache_nowrite
	lw	a2,scache_size
	blez	a2,2f
	lw	a3,scache_linesize

	vcacheop a0, a1, a2, a3, Hit_Invalidate_SD

	ALIGN3
2:	j	ra
END clean_scache_nowrite
	
/*
 * void clean_scache_nowrite_indexed (unsigned kva, size_t n)
 *
 * Invalidate indexed range in secondary cache
 */
	ALIGN3
LEAF clean_scache_nowrite_indexed
	mfc0	v0,C0_SR
	and	v1,v0,~SR_IE
	mtc0	v1,C0_SR

	mtc0	zero,C0_TAGLO
	mtc0	zero,C0_TAGHI

	lw	a2,scache_size
	blez	a2,2f
	lw	a3,scache_linesize

	icacheop a0, a1, a2, a3, Index_Store_Tag_SD

	ALIGN3
2:	mtc0	v0,C0_SR
	j	ra
END clean_scache_nowrite_indexed
.endc

.if true, (PROCESSOR&-16) == MIPS3000
/*
** ret_tlblo -- returns the 'entrylo' contents for the TLB
**	'c' callable - as ret_tlblo(index) - where index is the
**	tlb entry to return the lo value for - if called from assembly
**	language then index should be in register a0.
*/
FRAME ret_tlblo
	.set	noreorder
	mfc0	t0,C0_SR		# save sr
	nop
	and	t0,~SR_PE		# dont inadvertantly clear PE
	mtc0	zero,C0_SR		# clear interrupts
	mfc0	t1,C0_TLBHI		# save pid
	sll	a0,TLBINX_INXSHIFT	# position index
	mtc0	a0,C0_INX		# write to index register
	nop
	tlbr				# put tlb entry in entrylo and hi
	nop
	mfc0	v0,C0_TLBLO		# get the requested entry lo
	mtc0	t1,C0_TLBHI		# restore pid
	mtc0	t0,C0_SR		# restore status register
	j	ra
	nop
	.set	reorder
ENDFRAME(ret_tlblo)
.endc
.if true, (PROCESSOR&-16) == MIPS4000
/*
** ret_tlblo[01] -- returns the 'entrylo' contents for the TLB
**	'c' callable - as ret_tlblo(index) - where index is the
**	tlb entry to return the lo value for - if called from assembly
**	language then index should be in register a0.
*/
	ALIGN3
FRAME ret_tlblo0
	mfc0	t0,C0_SR		# save sr
	mtc0	zero,C0_SR		# clear interrupts
	mfc0	t1,C0_TLBHI		# save pid
	mtc0	a0,C0_INX		# write to index register
	.set noreorder
	nop; nop; nop; nop; nop; nop; nop; nop
	.set reorder
	tlbr				# put tlb entry in entrylo and hi
	.set noreorder
	nop; nop; nop; nop; nop; nop; nop; nop
	.set reorder
	mfc0	v0,C0_TLBLO0		# get the requested entry lo
	mtc0	t1,C0_TLBHI		# restore pid
	mtc0	t0,C0_SR		# restore status register
	j	ra
ENDFRAME ret_tlblo0

	ALIGN3
FRAME ret_tlblo1,sp,0,ra
	mfc0	t0,C0_SR		# save sr
	mtc0	zero,C0_SR		# clear interrupts
	mfc0	t1,C0_TLBHI		# save pid
	mtc0	a0,C0_INX		# write to index register
	.set noreorder
	nop; nop; nop; nop; nop; nop; nop; nop
	.set reorder
	tlbr				# put tlb entry in entrylo and hi
	.set noreorder
	nop; nop; nop; nop; nop; nop; nop; nop
	.set reorder
	mfc0	v0,C0_TLBLO1		# get the requested entry lo
	mtc0	t1,C0_TLBHI		# restore pid
	mtc0	t0,C0_SR		# restore status register
	j	ra
ENDFRAME ret_tlblo1

/*
** ret_pagemask(index) -- return pagemask contents of tlb entry "index"
*/
	ALIGN3
FRAME ret_pagemask
	mfc0	t0,C0_SR		# save sr
	mtc0	zero,C0_SR		# disable interrupts
	mfc0	t1,C0_TLBHI		# save current pid
	mtc0	a0,C0_INX		# drop it in C0 register
	.set noreorder
	nop; nop; nop; nop; nop; nop; nop; nop
	.set reorder
	tlbr				# read entry to entry hi/lo
	.set noreorder
	nop; nop; nop; nop; nop; nop; nop; nop
	.set reorder
	mfc0	v0,C0_PAGEMASK		# to return value
	mtc0	t1,C0_TLBHI		# restore current pid
	mtc0	t0,C0_SR		# restore sr
	j	ra
ENDFRAME ret_pagemask

/*
** ret_tlbwired(void) -- return wired register
*/
	ALIGN3
FRAME ret_tlbwired
	mfc0	v0,C0_WIRED
	j	ra
ENDFRAME ret_tlbwired
.endc

/*
** ret_tlbhi -- return the tlb entry high content for tlb entry
**			index
*/
	ALIGN3
FRAME ret_tlbhi
.if true, (PROCESSOR&-16) == MIPS3000
	.set	noreorder
	mfc0	t0,C0_SR		# save sr
	nop
	and	t0,~SR_PE
	mtc0	zero,C0_SR		# disable interrupts
	mfc0	t1,C0_TLBHI		# save current pid
	sll	a0,TLBINX_INXSHIFT	# position index
	mtc0	a0,C0_INX		# drop it in C0 register
	nop
	tlbr				# read entry to entry hi/lo
	nop
	mfc0	v0,C0_TLBHI		# to return value
	mtc0	t1,C0_TLBHI		# restore current pid
	mtc0	t0,C0_SR		# restore sr
	j	ra
	nop
	.set	reorder
.endc
.if true, (PROCESSOR&-16) == MIPS4000
	mfc0	t0,C0_SR		# save sr
	mtc0	zero,C0_SR		# disable interrupts
	mfc0	t1,C0_TLBHI		# save current pid
	mtc0	a0,C0_INX		# drop it in C0 register
	.set noreorder
	nop; nop; nop; nop; nop; nop; nop; nop
	.set reorder
	tlbr				# read entry to entry hi/lo0/lo1/mask
	.set noreorder
	nop; nop; nop; nop; nop; nop; nop; nop
	.set reorder
	mfc0	v0,C0_TLBHI		# to return value
	mtc0	t1,C0_TLBHI		# restore current pid
	mtc0	t0,C0_SR		# restore sr
	j	ra
.endc
ENDFRAME ret_tlbhi

/*
** ret_tlbpid() -- return tlb pid contained in the current entry hi
*/
	.align 3
FRAME ret_tlbpid
.if true, (PROCESSOR&-16) == MIPS3000
	.set	noreorder
	mfc0	v0,C0_TLBHI		# fetch tlb high 
	nop
	and	v0,TLBHI_PIDMASK	# isolate and position
	srl	v0,TLBHI_PIDSHIFT
	j	ra
	nop
	.set	reorder
.endc
.if true, (PROCESSOR&-16) == MIPS4000
	mfc0	v0,C0_TLBHI	# to return value
	and	v0,TLBHI_PIDMASK
	j	ra
.endc
ENDFRAME ret_tlbpid

/*
** Set current TLBPID. This assumes PID is positioned correctly in reg.
**			a0.
*/
	ALIGN3
FRAME set_tlbpid
.if true, (PROCESSOR&-16) == MIPS3000
	.set	noreorder
	sll	a0,TLBHI_PIDSHIFT
	mtc0	a0,C0_TLBHI
	j	ra
	nop
	.set	reorder
.endc
.if true, (PROCESSOR&-16) == MIPS4000
	mtc0	a0,C0_TLBHI
	j	ra
.endc
ENDFRAME set_tlbpid
.endc			; MIN_IDTC4000 == 0

/*
** tlbprobe(address, length) -- probe the tlb to see if address range is currently
**				mapped
**	a0 = vpn  - virtual page numbers are 0=0 1=0x1000, 2=0x2000...
**			virtual page numbers for the r3000 are in
**			entry hi bits 31-12
**	a1 = length in bytes to check
**	returns negative number if not in tlb, else returns C0_INX
*/
	ALIGN3
FRAME tlbprobe
.if true, (PROCESSOR&-16) == MIPS3000
  .if true, 1
	li	v0, -1
	ra
  .iff
	.set	noreorder
	mfc0	t0,C0_SR		/* fetch status reg */
	and	a0,TLBHI_VPNMASK	/* isolate just the vpn */
	and	t0,~SR_PE		/* don't inadvertantly clear pe */
	mtc0	zero,C0_SR 
	mfc0	t1,C0_TLBHI	
	sll	a1,TLBHI_PIDSHIFT	/* possition the pid */
	and	a1,TLBHI_PIDMASK
	or	a0,a1			/* build entry hi value */
	mtc0	a0,C0_TLBHI
	nop
	tlbp				/* do the probe */
	nop
	mfc0	v1,C0_INX
	li	v0,-1
	bltz	v1,1f
	nop
	sra	v0,v1,TLBINX_INXSHIFT	/* get index positioned for return */
1:
	mtc0	t1,C0_TLBHI		/* restore tlb hi */
	mtc0	t0,C0_SR		/* restore the status reg */
	j	ra
	nop
	.set	reorder
  .endc
.endc
.if true, (PROCESSOR&-16) == MIPS4000
	mfc0	t0, C0_SR		# save sr
	mfc0	t1, C0_TLBHI		# save current pid
	and	v0, t0, ~SR_IE		# disable interrups
	mtc0	v0, C0_SR		# disable interrupts
	addu	a1, a0			# compute an end address
	nor	v0, r0, r0		# assume failure (-1)
	bltu	a1, a0, 100f		# if end address is less than start, nfg
 .if true, (HOST_BOARD == PHOENIX_AD) || (HOST_BOARD == FLAGSTAFF) || (HOST_BOARD == SEATTLE) || (HOST_BOARD == VEGAS)
	move	v0, r0			# assume success
	li	v1, 0x80000000
	bltu	a0, v1, 5f		# below 0x80000000
	li	v1, 0x807FFFFF
	bleu	a1, v1, 100f		# in range, ok
	li	v1, 0xA0000000
	bltu	a0, v1, 5f		# below 0xA0000000
	li	v1, 0xA07FFFFF
	bleu	a1, v1, 100f		# in range, ok
	nor	v0, r0, r0		# assume failure (-1) again
5:
 .endc
	and	a0, TLBHI_VPN2MASK	# construct tlbhi for probe (with pid = 0)
	ALIGN3
10:	mtc0	a0, C0_TLBHI
	.set noreorder
	nop; nop; nop; nop; nop; nop; nop; nop
	tlbp				# probe entry to entry hi/lo0/lo1/mask
	nop; nop; nop; nop; nop; nop; nop; nop
	.set reorder
	mfc0	v0,C0_INX
	bltz	v0,100f			# if negative, there's no entry
	.set noreorder
	nop; nop; nop; nop; nop; nop; nop; nop
	tlbr				# read the 4 tlb registers 
	nop; nop; nop; nop; nop; nop; nop; nop
	.set reorder	
	mfc0	v1, C0_PAGEMASK		# get size of each of the two pages
	mfc0	a0, C0_TLBHI		# get starting virtual address of this region
	add	v1, 1<<13		# add 1 to the mask to get 2 pages worth of bytes
	add	a0, v1			# advance test address by 2 times page size to next region
	bltu	a0, a1, 10b		# continue testing if new address is less than limit
	ALIGN3
100:	mtc0	t1,C0_TLBHI		# restore current pid
	mtc0	t0,C0_SR		# restore sr
	j	ra
.endc
ENDFRAME tlbprobe

/*
** resettlb(index) Invalidate the  TLB entry specified by index
*/
	ALIGN3
FRAME resettlb
.if true, (PROCESSOR&-16) == MIPS3000
	.set	noreorder
	mfc0	t0,C0_TLBHI		# fetch the current hi 
	mfc0	v0,C0_SR		# fetch the status reg.
	li	t2,K0BASE&TLBHI_VPNMASK
	and	v0,~SR_PE		# dont inadvertantly clear PE
	and	v1, v0, ~SR_IE		# drop the IE bit
	mtc0	v1,C0_SR		# disable interrupts, but allow breakpoints
	mtc0	t2,C0_TLBHI		# set up tlbhi
	mtc0	zero,C0_TLBLO
	sll	a0,TLBINX_INXSHIFT
	mtc0	a0,C0_INX
	nop
	tlbwi				# do actual invalidate
	nop
	mtc0	t0,C0_TLBHI
	mtc0	v0,C0_SR
	j	ra
	nop
	.set	reorder
.endc
.if true, (PROCESSOR&-16) == MIPS4000
	li	t2,K0BASE&TLBHI_VPN2MASK
	mfc0	t0,C0_TLBHI		# save current TLBHI
	mfc0	v0,C0_SR		# save SR and disable interrupts
	mtc0	zero,C0_SR
	mtc0	t2,C0_TLBHI		# invalidate entry
	mtc0	zero,C0_TLBLO0
	mtc0	zero,C0_TLBLO1
	mtc0	a0,C0_INX
	.set noreorder
	nop; nop; nop; nop; nop; nop; nop; nop
	.set reorder
	tlbwi
	.set noreorder
	nop; nop; nop; nop; nop; nop; nop; nop
	.set reorder
	mtc0	t0,C0_TLBHI
	mtc0	v0,C0_SR
	j	ra
.endc
ENDFRAME resettlb

.if true, (PROCESSOR&-16) == MIPS3000
/*
** Setup TLB entry
**
** map_tlb(index, tlbhi, phypage)
** 	a0  =  TLB entry index
**	a1  =  virtual page number and PID
**	a2  =  physical page
*/
FRAME map_tlb
	.set	noreorder
	sll	a0,TLBINX_INXSHIFT
	mfc0	v0,C0_SR		# fetch the current status
	mfc0	a3,C0_TLBHI		# save the current hi
	and	v0,~SR_PE		# dont inadvertantly clear parity

	mtc0	zero,C0_SR
	mtc0	a1,C0_TLBHI		# set the hi entry
	mtc0	a2,C0_TLBLO		# set the lo entry 
	mtc0	a0,C0_INX		# load the index
	nop
	tlbwi				# put the hi/lo in tlb entry indexed
	nop
	mtc0	a3,C0_TLBHI		# put back the tlb hi reg 
	mtc0	v0,C0_SR		# restore the status register 
	j	ra
	nop
	.set	reorder
ENDFRAME map_tlb
.endc
.if true, (PROCESSOR&-16) == MIPS4000
/*
** Setup R4000 TLB entry
**
** map_tlb4000(mask_index, tlbhi, pte_even, pte_odd)
** 	a0  =  TLB entry index and page mask
**	a1  =  virtual page number and PID
**      a2  =  pte -- contents of even pte
**      a3  =  pte -- contents of odd pte
*/
	ALIGN3
FRAME map_tlb4000
	and	t2,a0,TLBPGMASK_MASK
	and	a0,TLBINX_INXMASK
	mfc0	t1,C0_TLBHI		# save current TLBPID
	mfc0	v0,C0_SR		# save SR and disable interrupts
	mtc0	zero,C0_SR
	mtc0	t2,C0_PAGEMASK		# set 
	mtc0	a1,C0_TLBHI		# set VPN and TLBPID
	mtc0	a2,C0_TLBLO0		# set PPN and access bits
	mtc0	a3,C0_TLBLO1		# set PPN and access bits
	mtc0	a0,C0_INX		# set INDEX to wired entry
	.set noreorder
	nop; nop; nop; nop; nop; nop; nop; nop
	.set reorder
	tlbwi				# drop it in
	.set noreorder
	nop; nop; nop; nop; nop; nop; nop; nop
	.set reorder
	mtc0	t1,C0_TLBHI		# restore TLBPID
	mtc0	v0,C0_SR		# restore SR
	j	ra
ENDFRAME map_tlb4000
.endc
.endc					;BOOT_ROM_CODE == 0
